import streamlit as st
import feedparser
from newspaper import Article
from openai import OpenAI

# --------------------------------------------------
# ê¸°ë³¸ ì„¤ì •
# --------------------------------------------------
st.set_page_config(page_title="ë‰´ìŠ¤ ìë™ìš”ì•½ë´‡", layout="wide")
st.title("ğŸ“° AI ê¸°ë°˜ ìë™ ë‰´ìŠ¤ ìš”ì•½ ì‹œìŠ¤í…œ")
st.write("ì¸í„°ë„·ì—ì„œ ìµœì‹  ë‰´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ê°€ì ¸ì™€ ìš”ì•½í•©ë‹ˆë‹¤.")

# OpenAI API ë¡œë“œ
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# --------------------------------------------------
# ìë™ ìˆ˜ì§‘ìš© í•œêµ­ ë‰´ìŠ¤ RSS ëª©ë¡
# --------------------------------------------------
AUTO_RSS = {
    "êµ¬ê¸€ ë‰´ìŠ¤(í•œêµ­)": "https://news.google.com/rss?hl=ko&gl=KR&ceid=KR:ko",
    "í•œêµ­ê²½ì œ": "https://www.hankyung.com/feed",
    "ë¨¸ë‹ˆíˆ¬ë°ì´": "http://rss.mt.co.kr/mt_news.xml",
    "YTN ì†ë³´": "https://www.ytn.co.kr/rss/news.xml",
}


# --------------------------------------------------
# (1) RSSì—ì„œ ìë™ìœ¼ë¡œ ìµœì‹  ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
# --------------------------------------------------
def get_auto_news_links(limit=8):
    links = []
    for name, url in AUTO_RSS.items():
        feed = feedparser.parse(url)
        for e in feed.entries[:3]:    # ë§¤ì²´ë‹¹ ìµœëŒ€ 3ê°œ
            links.append(e.link)
        if len(links) >= limit:
            break
    return links[:limit]


# --------------------------------------------------
# (2) Newspaper3kë¡œ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§
# --------------------------------------------------
def fetch_article(url):
    article = Article(url, language="ko")
    article.download()
    article.parse()

    return {
        "title": article.title,
        "text": article.text,
        "date": article.publish_date,
        "authors": article.authors,
        "url": url,
    }


# --------------------------------------------------
# (3) OpenAI GPT ìš”ì•½ ìƒì„±
# --------------------------------------------------
def summarize(text):
    prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ë³¸ë¬¸ì„ í•œêµ­ì–´ë¡œ í•µì‹¬ ìš”ì•½í•´ì¤˜.
- ê¸¸ì´: 3~5ë¬¸ì¥
- ì‚¬ì‹¤ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±
- ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ ì œê±°

ë³¸ë¬¸:
{text}
"""

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
    )

    return resp.choices[0].message["content"]


# --------------------------------------------------
# Streamlit UI ì‹œì‘
# --------------------------------------------------

st.sidebar.header("ì„¤ì •")
news_limit = st.sidebar.slider("ê°€ì ¸ì˜¬ ë‰´ìŠ¤ ê°œìˆ˜", 3, 20, 8)

if st.button("ğŸ“° ìµœì‹  ë‰´ìŠ¤ ìë™ ê°€ì ¸ì˜¤ê¸° & ìš”ì•½í•˜ê¸°"):

    st.info("ì—¬ëŸ¬ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ê¸°ì‚¬ë¥¼ ìë™ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤...")
    links = get_auto_news_links(limit=news_limit)

    st.success(f"ì´ {len(links)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
    st.write("----")

    # ê° ë§í¬ì— ëŒ€í•´ ì²˜ë¦¬
    for idx, url in enumerate(links):
        with st.spinner(f"{idx+1}/{len(links)} ê¸°ì‚¬ ë¶„ì„ ì¤‘..."):
            try:
                art = fetch_article(url)
            except Exception as e:
                st.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue

        # ê¸°ì‚¬ ì œëª©
        st.subheader(f"{idx+1}. {art['title']}")

        # ë©”íƒ€ ì •ë³´
        st.write(f"ğŸ—“ ë‚ ì§œ: {art['date']}")
        st.write(f"âœ ê¸°ì: {', '.join(art['authors'])}")
        st.write(f"[ğŸ”— ì›ë¬¸ ë³´ê¸°]({art['url']})")

        # ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°
        st.write("#### ğŸ“„ ë³¸ë¬¸ ì¼ë¶€:")
        preview = art["text"][:400] + "â€¦" if len(art["text"]) > 400 else art["text"]
        st.write(preview)

        # ìš”ì•½ ìƒì„±
        with st.spinner("ğŸ§  ìš”ì•½ ìƒì„± ì¤‘..."):
            summary = summarize(art["text"])

        st.write("### ğŸ§  ìš”ì•½")
        st.write(summary)

        st.markdown("---")

else:
    st.info("ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ìë™ìœ¼ë¡œ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ ìš”ì•½í•©ë‹ˆë‹¤.")
